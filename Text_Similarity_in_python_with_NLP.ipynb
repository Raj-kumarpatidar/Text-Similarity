{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWDs0VLcw1CbP4FO8azYt8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raj-kumarpatidar/Text-Similarity/blob/main/Text_Similarity_in_python_with_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use spacy library.\\\n",
        "spaCy is one of the popular and easy-to-use natural language processing library in Python. It helps in building applications that can process and get insights from large volumes of text. It can be used in task related to information extraction or natural language understanding systems, deep learning etc."
      ],
      "metadata": {
        "id": "Qi_fAI6yUwal"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3RfpXZSUrl3"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy (Natural Language Processing Library):\\\n",
        "spacy is a popular Python library for natural language processing (NLP). It's used for various NLP tasks like tokenization, part-of-speech tagging, named entity recognition, and more. Spacy also provides word vectors, which can be useful for various NLP applications.\\\n",
        "scikit-learn (Machine Learning Library):\\\n",
        "Scikit-learn is a widely used library for machine learning in Python. It includes a variety of tools for tasks such as classification, regression, clustering, dimensionality reduction, and more. here I use scikit-learn for text analysis.\\\n",
        "TfidfVectorizer (Term Frequency-Inverse Document Frequency Vectorizer):\\\n",
        "TfidfVectorizer is a feature extraction technique used in natural language processing. It converts a collection of raw documents (text) into a matrix of TF-IDF features. This allows me to represent text data numerically, where each document's words are weighted based on their importance in the document and across the entire corpus. It's often used for text classification and clustering tasks.\\\n",
        "cosine_similarity:\\\n",
        "cosine_similarity is a function in scikit-learn that calculates the cosine similarity between two or more vectors. In the context of text analysis, here,I use it to measure the similarity between text documents. It's commonly used for tasks like finding similar documents, clustering related documents, or recommendation systems. \\\n",
        "\n",
        "The typical workflow for using these libraries in text analysis might involve the following steps:\n",
        "\n",
        "Data Preprocessing:\n",
        "\n",
        "Use Spacy for text preprocessing tasks like tokenization, stemming, and removing stop words.\n",
        "Prepare your text data by cleaning and structuring it for analysis.\n",
        "\n",
        "Feature Extraction:\n",
        "\n",
        "Use TfidfVectorizer to convert the text data into TF-IDF feature vectors. This step transforms the text data into a format suitable for machine learning.\n",
        "\n",
        "Cosine Similarity Calculation:\n",
        "\n",
        "Calculate the cosine similarity between pairs of documents using the cosine_similarity function. The result will be a similarity score indicating how closely related the documents are.\n",
        "\n"
      ],
      "metadata": {
        "id": "tJfHOeVtUvkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "FNJMAli-U47i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By loading the \"en_core_web_sm\" model using spacy.load(), you are creating an NLP object named nlp. This NLP object contains various language processing components, including tokenizers, part-of-speech taggers, named entity recognizers, dependency parsers, and word vectors. It allows you to process and analyze text using these pre-trained components.\n",
        "\n",
        "\"en_core_web_sm\" is the name of a specific pre-trained model in SpaCy. In this case, it's the \"English (en) core web small (core_web_sm)\" model.\n",
        "\n"
      ],
      "metadata": {
        "id": "GzUwEgeEU8tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(tokens)\n"
      ],
      "metadata": {
        "id": "EKbC0YffVAVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After processing the text, this line creates a list called tokens. It iterates over each token in the processed doc. For each token, it does the following:\n",
        "\n",
        "token.text.lower() extracts the lowercase form of the token. This helps standardize the text and make it case-insensitive.\n",
        "not token.is_stop checks if the token is not a stop word. Stop words are commonly used words like \"the,\" \"is,\" \"and\" that are often removed in text preprocessing because they don't typically add much meaning to the text.\n",
        "not token.is_punct checks if the token is not a punctuation mark. Punctuation marks like periods, commas, and exclamation marks are often removed from text during preprocessing.\n",
        "So, this line effectively creates a list of lowercase words from the input text, excluding stop words and punctuation marks.\n",
        "Finally, the tokens list is joined together into a single string. The words are separated by spaces, creating a preprocessed version of the text."
      ],
      "metadata": {
        "id": "-w_ePmOT38Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cosine_similarity(documents):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "    return similarity_matrix\n"
      ],
      "metadata": {
        "id": "wlxfcwA32-Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A TF-IDF vectorizer is created using TfidfVectorizer. This vectorizer is then fitted to the input documents using the fit_transform method. TF-IDF is a technique used to convert text data into numerical vectors, where each vector represents a document, and the elements of the vector correspond to the importance of words in the document relative to the entire collection of documents. It helps to emphasize important words while downweighting common words (stop words).\\\n",
        " cosine_similarity function from scikit-learn to calculate the cosine similarity between the documents. The cosine_similarity function takes the TF-IDF matrix as input and computes the pairwise cosine similarity between the rows of this matrix\\\n",
        " this function takes a list of text documents, converts them into TF-IDF vectors, and then calculates the cosine similarity between these vectors. The resulting similarity matrix provides a measure of how similar or related each pair of documents is, which can be useful for various text analysis tasks such as document retrieval, clustering, or recommendation systems."
      ],
      "metadata": {
        "id": "UejUFF2x4XHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_word_embedding_similarity(doc1, doc2):\n",
        "    tokens1 = nlp(doc1)\n",
        "    tokens2 = nlp(doc2)\n",
        "\n",
        "    vec1 = tokens1.vector\n",
        "    vec2 = tokens2.vector\n",
        "\n",
        "    similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
        "    return similarity\n"
      ],
      "metadata": {
        "id": "7GfhcawF3Azt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " defines a function named calculate_word_embedding_similarity that takes two arguments, doc1 and doc2. These arguments should be strings representing the two documents for which you want to calculate similarity.\\\n",
        "  the input documents are processed using the nlp object, which is the SpaCy language model. This step tokenizes the text and performs various linguistic analyses.\\\n",
        "  After tokenization, this code extracts the word embedding vectors for both doc1 and doc2. In SpaCy, the tokens.vector attribute provides a dense vector representation of the entire document based on the word embeddings. These vectors capture the semantic meaning of the words in the document.\\\n",
        "  With the word embedding vectors obtained, the code then calculates the cosine similarity between these vectors using the cosine_similarity function from scikit-learn. It compares vec1 and vec2 to determine how similar or related the two documents are.\\\n",
        "   this function leverages SpaCy's word embeddings to calculate the cosine similarity between two documents. It measures the semantic similarity between the documents based on the distributional properties of words in their respective embeddings. This can be useful for various NLP tasks, such as document similarity, recommendation systems, or information retrieval."
      ],
      "metadata": {
        "id": "2qxVRGUL8HTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = \"This is the first document.\"\n",
        "doc2 = \"Here is the second document.\"\n",
        "doc3 = \"A different document, unrelated to the others.\"\n",
        "\n",
        "# doc1=\"raj\"\n",
        "# doc2=\"ravina\"\n",
        "# doc3=\"ravi\"\n",
        "\n",
        "documents = [doc1, doc2, doc3]\n",
        "\n",
        "# Preprocess the documents\n",
        "processed_documents = [preprocess(doc) for doc in documents]\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_matrix = calculate_cosine_similarity(processed_documents)\n",
        "\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "print(similarity_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1ejpK103JSl",
        "outputId": "b7f3ed74-87b0-45f8-95db-a45036d57130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Matrix:\n",
            "[[1.         0.50854232 0.38537163]\n",
            " [0.50854232 1.         0.19597778]\n",
            " [0.38537163 0.19597778 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have three documents (doc1, doc2, and doc3) stored as strings.\n",
        "\n",
        "You create a list called documents that contains these three documents.\n",
        "\n",
        "You then preprocess these documents using the preprocess function. The preprocessed documents are stored in the processed_documents list.\n",
        "\n",
        "Next, you calculate the cosine similarity between the preprocessed documents using the calculate_cosine_similarity function. The result is stored in the similarity_matrix.\n",
        "\n",
        "Finally, you print the cosine similarity matrix, which represents the pairwise similarity between the documents."
      ],
      "metadata": {
        "id": "tMCv-0Ks80fT"
      }
    }
  ]
}